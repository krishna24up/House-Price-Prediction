import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer

# ğŸ“‚ Load Dataset
df = pd.read_csv("BostonHousing.csv")  # Update path if needed

# âœ… Handle missing values
imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# ğŸ‘ï¸ Basic Overview
print("ğŸ” Missing values (after imputation):\n", df_imputed.isnull().sum())
print("\nğŸ“‹ Sample Data:")
print(df_imputed.head())

# ğŸ“Š Correlation Heatmap
plt.figure(figsize=(12,8))
sns.heatmap(df_imputed.corr(), annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

# ğŸ“ˆ Pairplot for important features
important_features = ['RM', 'LSTAT', 'PTRATIO', 'TAX', 'MEDV']
sns.pairplot(df_imputed[important_features])
plt.show()

# ğŸ§¹ Feature/Target split
X = df_imputed.drop('MEDV', axis=1)
y = df_imputed['MEDV']

# ğŸ§ª Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ğŸ“ Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ğŸ¤– Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# ğŸ“Š Evaluation - Linear
print("\nğŸ“ˆ Linear Regression:")
print("MSE:", mean_squared_error(y_test, y_pred_lr))
print("RÂ²:", r2_score(y_test, y_pred_lr))

# ğŸ¯ Plot Actual vs Predicted (LR)
plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred_lr, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted Prices (Linear Regression)")
plt.show()

# ğŸŒ² Random Forest
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# ğŸ“Š Evaluation - Random Forest
print("\nğŸŒ² Random Forest:")
print("MSE:", mean_squared_error(y_test, y_pred_rf))
print("RÂ²:", r2_score(y_test, y_pred_rf))

# ğŸ” Feature Importance
importances = pd.Series(rf.feature_importances_, index=X.columns)
importances.sort_values().plot(kind='barh', figsize=(10,6), color='green')
plt.title("Feature Importances (Random Forest)")
plt.show()

# ğŸ§¾ Predict with user input (only important features)
print("\n--- ğŸ”® Predict New House Price (Only Important Features) ---")

# Only ask for top features
top_features = ['RM', 'LSTAT', 'PTRATIO', 'TAX']
mean_values = X.mean()
user_input_dict = {}

for feat in top_features:
    val = float(input(f"Enter value for {feat}: "))
    user_input_dict[feat] = val

# Build final input with means + user values
final_input = []
for feat in X.columns:
    if feat in user_input_dict:
        final_input.append(user_input_dict[feat])
    else:
        final_input.append(mean_values[feat])

# Scale and predict
user_input_arr = np.array(final_input).reshape(1, -1)
user_input_scaled = scaler.transform(user_input_arr)

lr_pred = lr.predict(user_input_scaled)[0]
rf_pred = rf.predict(user_input_scaled)[0]

print("\nğŸ“¢ Predicted House Price:")
print(f"ğŸ”¹ Linear Regression: ${lr_pred:.2f}")
print(f"ğŸ”¹ Random Forest: ${rf_pred:.2f}")
